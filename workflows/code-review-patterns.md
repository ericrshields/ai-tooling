# AI Code Review Patterns

Hub for comprehensive patterns on automated code review using LLMs and AI agents based on 2026 industry research.

---

## Overview

AI-powered code review has matured significantly by 2026. Modern approaches combine prompt engineering, multi-dimensional parallel analysis, and self-healing workflows to achieve both speed and comprehensiveness.

**Key Principle**: AI should be treated as a powerful pair programmer that requires clear direction, context, and oversight rather than autonomous judgment.

**Architecture**: Run specialized review agents in parallel (5-6x faster), each focused on a specific dimension (security, performance, maintainability, etc.), then synthesize results into unified actionable feedback.

---

## Core Patterns

### Prompt Engineering

Effective prompts are the foundation of quality AI code review.

**Key Techniques**:
- **Role Prompting**: Assign explicit role and expertise level
- **Few-Shot Learning**: Provide good/bad examples with explanations
- **Chain-of-Thought**: Request step-by-step reasoning
- **Self-Consistency**: Run multiple reviews, report consensus

**When to Fine-Tune**: High volume reviews, consistent codebase, sufficient training data (hundreds of PRs).

**When to Use Prompts**: Limited data, diverse codebases, rapid evolution, prototyping.

See [patterns/code-review/prompt-engineering.md](../patterns/code-review/prompt-engineering.md) for complete prompt patterns and examples.

### Multi-Dimensional Parallel Review

Run specialized agents in parallel, each focusing on one dimension, then synthesize.

**Review Dimensions**:
1. **Security**: OWASP Top 10, auth/authz, input validation, secrets
2. **Performance**: Algorithmic complexity, memory, database queries, caching
3. **Maintainability**: Code complexity, DRY, SOLID, naming, organization
4. **Accessibility**: WCAG compliance, semantic HTML, ARIA, keyboard nav (UI code)
5. **Style & Standards**: Linting, formatting, type safety, documentation
6. **Test Coverage**: Tests present, edge cases, test quality

**Benefits**: 5-6x faster than sequential, reduced iteration churn, comprehensive coverage.

**Pattern**: Fan-Out → Parallel Execution → Gather → Synthesis

See [patterns/code-review/parallel-review.md](../patterns/code-review/parallel-review.md) for implementation details.

### CI/CD Integration

Integrate AI code review directly into deployment pipelines.

**Capabilities**:
- Intelligent test selection (run only relevant tests)
- Autonomous quality gates (context-aware decisions)
- Business-driven deployment (consider revenue, customer impact)
- Self-healing workflows (generate → review → fix loop)

**Performance**: 78% deployment time reduction while improving quality.

**Tools**: TestSprite, Testim, Functionize, Applitools, Testsigma.

See [patterns/code-review/ci-cd-integration.md](../patterns/code-review/ci-cd-integration.md) for pipeline integration patterns.

---

## Quick Start

### For Simple Review

**Use zero-shot with context:**
```
"Review this code for [specific concern]. Focus on [priorities]."
```

### For Comprehensive Review

**Run parallel multi-dimensional review:**
1. Distribute code to 6 specialized agents (security, performance, etc.)
2. Run all agents in parallel
3. Synthesize results into unified report
4. Prioritize by severity

### For Production Integration

**Integrate into CI/CD:**
1. Configure intelligent test selection
2. Set up autonomous quality gates
3. Define business-driven deployment criteria
4. Monitor and iterate on thresholds

---

## Best Practices

**Treat LLM as Pair Programmer**:
- Provide clear instructions and sufficient context
- Validate recommendations, don't blindly accept
- Maintain human oversight

**Define Clear Criteria**:
- What issues to look for
- Acceptable/unacceptable patterns
- Risk tolerance
- Priority order (security > performance > style)

**Validate Quality**:
- Spot-check AI reviews vs human reviews (10% sample)
- Track false positives and negatives
- Refine prompts based on errors
- Fine-tune models when data available

**Parallelize**:
- Run independent reviews simultaneously
- Minimize iteration churn
- Faster feedback to developers

---

## When to Use What

| Scenario | Recommended Approach |
|----------|---------------------|
| Quick PR review | Zero-shot with clear context + constraints |
| Comprehensive quality check | Multi-dimensional parallel review (all 6 dimensions) |
| Security-focused | Role prompting + security-specific checklist |
| Consistency check | Comparison pattern (reference existing code) |
| Migration review | Historical context pattern (compare to v1) |
| Production pipeline | Full CI/CD integration with intelligent selection |

---

## AI-on-AI Review

**Emerging practice**: One AI model reviews code generated by another.

**Approaches**:
- Different models (GPT-4 generates, Claude reviews)
- Different contexts (optimistic generator, critical reviewer)
- Multi-pass (security pass, performance pass, maintainability pass)

**Benefit**: Catches issues one model missed, reduces confirmation bias.

---

## SPECIALIZED REVIEW AGENTS

Implement multi-dimensional parallel review with specialized agents (5-6x faster than sequential):

### Core Review Agents

**security-reviewer**:
- OWASP Top 10 validation
- Authentication/authorization checks
- Input validation, SQL/XSS/command injection detection
- Secrets scanning, secure defaults validation
- Generates security-specific findings

**performance-reviewer**:
- Algorithmic complexity analysis (Big O)
- Database query optimization (N+1, missing indexes)
- Memory leak detection, caching opportunities
- Bundle size impact (frontend)
- Generates performance-specific findings

**maintainability-reviewer**:
- Code complexity metrics (cyclomatic, cognitive)
- DRY/SOLID/KISS/YAGNI adherence
- Naming clarity, code organization
- Documentation completeness
- Generates maintainability-specific findings

**accessibility-reviewer** (UI code):
- WCAG 2.1/2.2 compliance
- Semantic HTML validation
- ARIA attributes correctness
- Keyboard navigation support
- Screen reader compatibility
- Generates accessibility-specific findings

**style-reviewer**:
- Linting violations (ESLint, Pylint, etc.)
- Code formatting (Prettier, Black, etc.)
- Type safety (TypeScript, mypy, etc.)
- Naming conventions, comment quality
- Generates style-specific findings

**test-quality-reviewer**:
- Test coverage percentage and gaps
- Test brittleness detection (implementation-specific tests)
- Edge case coverage validation
- Test maintainability assessment
- Generates test-quality-specific findings

### Synthesis Agents

**review-synthesizer**:
- Collects findings from all parallel reviewers
- Deduplicates overlapping issues
- Prioritizes by severity (blocking → warning → suggestion)
- Generates unified, actionable report
- Identifies conflicting recommendations

**comparison-agent**:
- Compares code to existing patterns in codebase
- Flags inconsistencies in style/approach
- Suggests alignment with established conventions
- Validates framework best practices adherence

### Meta-Review Agents

**reviewer-quality-checker**:
- Validates review agent outputs
- Detects false positives/negatives
- Measures review quality metrics
- Suggests prompt refinements

**ai-on-ai-reviewer**:
- Reviews code generated by AI
- Uses different model than generator
- Critical/skeptical stance vs. optimistic generator
- Reduces confirmation bias

### Integration Agents

**ci-reviewer**:
- Integrates review into CI/CD pipeline
- Intelligent test selection (run only relevant tests)
- Autonomous quality gate decisions
- Self-healing workflow orchestration

**pr-commenter**:
- Posts review findings as PR comments
- Groups related issues together
- Provides code suggestions (GitHub suggestions format)
- Tags severity levels, links to docs

### Supporting Agents

**context-gatherer**:
- Collects relevant code context for reviewers
- Extracts related files, tests, documentation
- Gathers historical changes, related PRs
- Provides architecture diagrams, patterns

**fix-generator**:
- Generates fixes for identified issues
- Creates code suggestions for reviewers
- Validates fixes don't introduce new issues
- Respects existing code patterns

---

## Sources

- [AI Prompts for Code Review: Catch Bugs, Improve Structure, Ship Better Code](https://5ly.co/blog/ai-prompts-for-code-review/)
- [Fine-tuning and prompt engineering for large language models-based code review automation](https://www.sciencedirect.com/science/article/pii/S0950584924001289)
- [My LLM coding workflow going into 2026](https://medium.com/@addyosmani/my-llm-coding-workflow-going-into-2026-52fe1681325e)
- [Best practices for LLM prompt engineering](https://www.palantir.com/docs/foundry/aip/best-practices-prompt-engineering)
- [AI Agents in CI/CD Pipelines for Continuous Quality](https://www.mabl.com/blog/ai-agents-cicd-pipelines-continuous-quality)
- [Ultimate Guide - The Best AI CI/CD Testing Automation Tools of 2026](https://www.testsprite.com/use-cases/en/the-top-ai-ci-cd-testing-automation-tools)
- [Autonomous Quality Gates: AI-Powered Code Review](https://www.augmentcode.com/guides/autonomous-quality-gates-ai-powered-code-review)

---

**Related Documentation**:
- [patterns/code-review/prompt-engineering.md](../patterns/code-review/prompt-engineering.md) - Prompt patterns and techniques
- [patterns/code-review/parallel-review.md](../patterns/code-review/parallel-review.md) - Multi-dimensional review implementation
- [patterns/code-review/ci-cd-integration.md](../patterns/code-review/ci-cd-integration.md) - CI/CD pipeline integration
- [../rules/multi-agent-orchestration.md](../rules/multi-agent-orchestration.md) - Agent coordination
- [../rules/observability-patterns.md](../rules/observability-patterns.md) - Monitoring and evaluation
